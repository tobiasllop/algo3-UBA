# Práctica 1

### Ejercicio 3
Se nos dice que la matriz es simetrica por lo que primero la completaremos:

$$
\begin{pmatrix}
 0 & 10 & 10 & 1 \\
 10 & 0 & 5 & 2 \\
 10 & 5 & 0 & 1 \\
 1 & 2 & 1 & 0 \\
\end{pmatrix}
$$

#### a.

     def max_subset_sum(M, n, k, i=0, curr_sum=0, curr_subset=[]):
         # Caso base: si ya tenemos k elementos en curr_subset
         if len(curr_subset) == k:
             return curr_sum
     
         # Si ya no quedan más elementos que agregar
         if i == n:
             return 0
     
         # Caso 1: No agregar el elemento i al subconjunto
         omit = max_subset_sum(M, n, k, i+1, curr_sum, curr_subset)
     
         # Caso 2: Agregar el elemento i al subconjunto
         curr_subset.append(i)
         subset_sum = curr_sum
         for j in curr_subset:
             subset_sum += M[i][j] + M[j][i]
         include = max_subset_sum(M, n, k, i+1, subset_sum, curr_subset)
         curr_subset.pop()
     
         # Devolver el máximo de los dos casos
         return max(omit, include)
 
     # Función principal
     def max_subset_matrix_sum(M, k):
         n = len(M)
         return max_subset_sum(M, n, k)
             
#### b.
La complejidad temporal de este algoritmo es O(n * 2^n), ya que en cada nivel del árbol de recursión hay n posibles elementos para agregar al subconjunto, y hay un total de 2^n subconjuntos posibles.

### Ejercicio 4
Escribi el arbol de BT en el cuaderno.

a. 

    import sys

    def encuentra_solucion_optima(D):
        n = len(D)
        solucion_optima = []
        costo_minimo = sys.maxsize

        def backtrack(permutacion, costo, indice):
            nonlocal costo_minimo, solucion_optima

            # Si se han explorado todas las posiciones, actualizar la solución óptima
            if indice == n:
                costo += D[permutacion[-1]][permutacion[0]]  # Agregar el costo de regresar al inicio
                if costo < costo_minimo:
                    costo_minimo = costo
                    solucion_optima = permutacion[:] # Hacer una copia de la permutación
                return

            # Explorar todas las posibilidades para la posición actual
            for i in range(n):
                if i not in permutacion:
                    permutacion.append(i)
                    backtrack(permutacion, costo + D[permutacion[-2]][permutacion[-1]], indice + 1)
                    permutacion.pop()

        backtrack([], 0, 0)
        return solucion_optima, costo_minimo

#### b. 
Complejidad temporal es O(n!) ya que hay que calcular todas las permutaciones y espacial es O(n)
    
#### c. Un ejemplo de poda por optimalidad:
Supongamos que estás intentando resolver el problema mediante backtracking y ya has encontrado una permutación $\pi$ con un costo total $C$. Ahora estás explorando otra permutación parcial $\pi'$ y calculas el costo total parcial $C'$ de esta permutación parcial. Si $C' \geq C$, entonces puedes descartar esta rama de búsqueda (realizar una "poda"), ya que cualquier extensión de $\pi'$ resultará en un costo total mayor o igual a $C'$, y por lo tanto, nunca será mejor que la solución que ya tienes.

Ejemplo específico:
Imagina que ya has encontrado una solución $\pi = (1, 2, 3, 4)$ con un costo de $10$. Ahora estás explorando una solución parcial $\pi' = (2, 1, ...)$ y calculas el costo parcial de ir de 2 a 1 basado en tu matriz $D$, que resulta ser 11. Dado que este costo parcial ya es mayor que el costo total de tu mejor solución hasta ahora, puedes concluir de inmediato que continuar explorando esta rama no conducirá a una mejor solución. Por lo tanto, puedes podar esta rama y volver a una decisión anterior para intentar una ruta diferente.

Cómo demostrar que esta poda es correcta:
Para demostrar que una poda por optimalidad es correcta, necesitas argumentar que no estás descartando ninguna solución que pudiera ser mejor que tu mejor solución actual. En este caso, el argumento se basa en el hecho de que si el costo de una solución parcial ya excede el costo de la mejor solución completa encontrada hasta ahora, entonces, por definición, esa solución parcial (y cualquier extensión de ella) no puede resultar en una solución óptima

## Ejercicio 6 Optipago
#### Ej a. 
$$
Cc(c',q) = 
\begin{cases}
(0,0) & \text{si } c \leq 0 \\
(\infty, \infty) & \text{si } c > 0 \land B = \emptyset\\
\min\{Cc(B - b_{n}, c), Cc(B-b_{n}, c -b_{n}) + (0,1)\} & \text{ si } c > 0 \land B \neq \emptyset 
\end{cases}
$$


#### Ej b (preguntar sobre como se comparan los costos y en que estructura se esta almacenando cada llamada recursiva)

    def Cc(B, c, n): 
        if c <= 0: 
            return (0, 0) 
        if c > 0 and n == 0: 
            return (float('inf'), float('inf'))

        b_sin_ultimo, q_sin_ultimo = Cc(B, c, n-1)
        b_con_ultimo, q_con_ultimo = Cc(B, c - B[n-1],n-1)

        b_con_ultimo += B[n-1]  # Ajustamos el valor de pagar con el último billete
        q_con_ultimo += 1  # Ajustamos la cantidad de billetes al sumar el actual

        # Decidir cuál de los dos caminos tomar basándonos en el exceso y en la cantidad de billetes
        if b_sin_ultimo < b_con_ultimo or (b_sin_ultimo == b_con_ultimo and q_sin_ultimo <= q_con_ultimo):
            return (b_sin_ultimo, q_sin_ultimo)
        else:
            return (b_con_ultimo, q_con_ultimo)

 Complejidad: O(2^n) ya que en cada llamada recursiva tenemos dos opciones, incluir o no incluir el ultimo billete
 #### Ej c preguntar (no veo donde esta la superposicion)


 #### Ej d. e. f.
 Este código inicializa una matriz de memoización memo de tamaño $(n+1) \times (c+1)$ con todos sus elementos en None, lo que indica que aún no se han calculado esos valores. Luego, en cada llamada recursiva, verifica si el resultado ya se encuentra en memo. Si es así, devuelve el resultado almacenado. Si no, realiza el cálculo y almacena el resultado en memo antes de retornarlo. Esto asegura que cada combinación de n y c se calcula y almacena una sola vez, lo cual reduce significativamente el tiempo de ejecución del algoritmo.

Recuerda que la memoización es especialmente útil en problemas donde hay una gran superposición de subproblemas, como es el caso aquí. Al almacenar los resultados intermedios, evitamos la recomputación, lo cual hace al algoritmo mucho más eficiente.

    def Cc_memo(B, c, n, memo): 
        if c <= 0: 
            return (0, 0) 
        if c > 0 and n == 0: 
            return (float('inf'), float('inf')) 
        
        if memo[n][c] is not None: 
            # Verificamos si el resultado ya está en memo 
            return memo[n][c]

        b_sin_ultimo, q_sin_ultimo = Cc_memo(B, c, n-1, memo)
        b_con_ultimo, q_con_ultimo = Cc_memo(B, c - B[n-1], n-1, memo)

        b_con_ultimo += B[n-1]
        q_con_ultimo += 1

        if b_sin_ultimo < b_con_ultimo or (b_sin_ultimo == b_con_ultimo and q_sin_ultimo <= q_con_ultimo):
            memo[n][c] = (b_sin_ultimo, q_sin_ultimo)  # Guardamos el resultado en memo
        else:
            memo[n][c] = (b_con_ultimo, q_con_ultimo)  # Guardamos el resultado en memo
        return memo[n][c]
Complejidad: O(n*c)


## Ejercicio 7 AstroTrade
#### Ej b.
$$
mgn(j,c) = 
\begin{cases}
 0 & \text{si } j = 0 \land c = 0\\
-\infty & \text{si } c < 0 \lor c > j\\
\max\{mgn(j-1, c-1) - p_{j}, mgn(j-1, c), mgn(j-1, c+1) + p_{j}\} & \text{ si } 0 \leq c \leq j 
\end{cases}
$$

#### Ej c.
La respuesta al problema, utilizando la formulación recursiva desarrollada, sería el valor de la función para el último día $n$ con $c = 0$ asteroides. En otras palabras, es el valor máximo que Astro Void puede obtener al final del período dado, siguiendo las operaciones óptimas de compra y venta.

#### Ej d.

    def mgn(p,j-1,c,memo):
        if j == 0 and c == 0:
           return 0
        if j == 0 and c != 0:
            return -float(inf)
        if c < 0 or c > j:
            return -float(inf)
        
        if memo[j][c] is not None:
            return memo[j][c]

        compra = mgn(p,j-1,c-1, memo) - p[j]
        igual = mgn(p, j-1, c, memo)
        venta = mgn(p, j-1, c+1, memo) + p[j]

        memo[j][c] = max(compra,igual, venta)
        return memo[j][c]

    n = len(p) # Asumiendo que p es tu lista de precios 
    memo = [[None] * (n+1) for _ in range(n+1)] 
    resultado = mgn(p, n, 0, memo)

Tanto la complejidad temporal como la espacial del algoritmo son (O(n^2)), donde (n) es el número de días.

### Definciones útiles
**Complejidad Temporal**:

La complejidad temporal de un algoritmo se refiere a la cantidad de tiempo que tarda este en ejecutarse, en función del tamaño de la entrada. Se mide generalmente en términos de la cantidad de operaciones básicas (como sumas, multiplicaciones, comparaciones, etc.) que el algoritmo necesita realizar. La idea es tener una medida aproximada de cuánto se demora el algoritmo en correr, sin depender de factores externos como el hardware específico donde se ejecuta.

Por ejemplo, si un algoritmo recorre todos los elementos de una lista para encontrar un valor, y cada elemento se verifica una sola vez, la complejidad temporal es lineal, ya que el tiempo de ejecución crece de manera proporcional al número de elementos. Usualmente, esto se denota como O(n), donde n es el tamaño de la entrada.

**Complejidad Espacial**:

Por otro lado, la complejidad espacial se refiere a la cantidad de memoria que un algoritmo necesita para su ejecución, también en función del tamaño de la entrada. Esto incluye tanto la memoria utilizada para almacenar las variables y estructuras de datos temporales como la necesaria para la entrada y salida del algoritmo.

Por ejemplo, si necesitas una matriz de tamaño n x n para solucionar un problema, la complejidad espacial será cuadrática, ya que el espacio necesario crece con el cuadrado del tamaño de la entrada (n^2). Esto se denota como O(n^2).

En resumen, la complejidad temporal nos da una idea de qué tan rápido corre un algoritmo, mientras que la complejidad espacial nos dice cuánta memoria necesitará. Ambos son aspectos cruciales a considerar al diseñar algoritmos, especialmente para aplicaciones en entornos con recursos limitados.